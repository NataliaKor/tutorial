{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpJ5FnQFw7rYlbrlP/n46K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataliaKor/tutorial/blob/main/point_PE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tutorial**: Three ways to do parametere estimation with Machine Learning.  \n",
        "*Example of Galactic Binary.*\n",
        "\n",
        "---\n",
        "\n",
        "## Natalia Korsakova *korsakova@apc.in2p3.fr*\n",
        "\n",
        "In this tutorial we build a simple point parameter estimation neural network for galactic binaries.\n",
        "\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Build a set of training waveforms.** We use the FastGB waveforms package.\n",
        "2. **Build CNN which takes in data and returns values of parameters.** This is usually called point parameter estimation (regression in the field of machine learning).\n",
        "3. **Train the model from waveform to parameter**,\n",
        "    - During training, we add noise to waveforms to make simulated data.\n",
        "5. **Evaluate** on test data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nABW5fNky2u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will generate waveform with the FastGB GPU accelerate waveform (https://github.com/mikekatz04/GBGPU). Lightweight version of FastGB is available here:\n"
      ],
      "metadata": {
        "id": "fP8Oc_yO2cdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting library not included in colab.\n",
        "!pip install corner"
      ],
      "metadata": {
        "id": "vfGCSomhcPpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE0TcWibfIjp"
      },
      "outputs": [],
      "source": [
        "# Waveform generation libary based on FastGB plus noise curve for LISA.\n",
        "!git clone https://github.com/NataliaKor/GBGPU.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from GBGPU.gbgpu.gbgpu import GBGPU\n",
        "from GBGPU.gbgpu.noisemodel import AnalyticNoise"
      ],
      "metadata": {
        "id": "q1ihcCFJf4dt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the code hardware agnostic.\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch import nn\n",
        "from torch import distributions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import corner\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Making code agnostic to CPU/GPU\n",
        "def std_get_wrapper(arg):\n",
        "    return arg\n",
        "\n",
        "def cuda_get_wrapper(arg):\n",
        "    return arg.get()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "   import cupy as cp\n",
        "   gpu = True\n",
        "   get_wrapper = cuda_get_wrapper\n",
        "   dev = \"cuda:0\"\n",
        "   dtype = torch.cuda.FloatTensor\n",
        "else:\n",
        "   import numpy as cp\n",
        "   gpu = False\n",
        "   get_wrapper = std_get_wrapper\n",
        "   dev = \"cpu\"\n",
        "   dtype = torch.FloatTensor\n",
        "\n",
        "print(gpu)"
      ],
      "metadata": {
        "id": "W1xIvjwlf5d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training data\n",
        "\n",
        "### Signal model\n",
        "\n",
        "The galactic binary waveforms that we use depend on 8 parameters $\\theta$:\n",
        "* Amplitude `amp`\n",
        "* Initial frequency `f0` (Hz)\n",
        "* Initial time derivative of frequency `fdot` (Hz/s)\n",
        "* Initial phase angle of gravitational wave `phi0` (radians)\n",
        "* Inclination of the orbit `iota` (radians)\n",
        "* Ecliptic longitude `lam` (radians)\n",
        "* Ecliptic lattitude `beta` (radians)\n",
        "* Polarization angle `psi` (radians)\n",
        "\n",
        "The FastGB package then gives us waveforms $h(\\theta) = (A(\\theta), E(\\theta))$ where $A$, $E$ are TDI channels.\n",
        "\n",
        "Initially, we perform inference over `beta` and `lam`, holding the remaining parameters at fixed values. Training data must be drawn from the prior, and we choose uniform `sin(beta)`, `lam` priors over some range.\n",
        "\n",
        "### Noise\n",
        "\n",
        "Ultimately we must train on simulated data, which also include noise,\n",
        "$$\n",
        "d = h(\\theta) + n, \\qquad n \\sim p_{S_n}(n).\n",
        "$$\n",
        "The noise is taken to be stationary Gaussian with some power spectral density $S_n$.\n",
        "\n",
        "Rather than creating complete simulated data sets in advance of training, **we only prepare the waveforms in advance, and we add noise realizations during training.** The reason for this is that we would like the training dataset to be as large as possible to reduce the risk of overfitting. Noise is fast to sample, so this can be done during training, and doing so effectively makes the training set much larger. Generally waveforms are slower to generate, so general practice is to re-use them in each epoch. However with a very fast waveform model (e.g., using GPU acceleration) training could be further improved by generating all data on the fly."
      ],
      "metadata": {
        "id": "OS4ZnUQl72Q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter sampling\n",
        "\n",
        "Initially, we construct a dataset consisting of $10^4$ waveforms, all drawn from the prior."
      ],
      "metadata": {
        "id": "qGC7tE578yim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of the training set.\n",
        "num_samples = 10000\n",
        "\n",
        "# We choose a very narrow frequency range.\n",
        "f0_lower = 0.010062\n",
        "f0_upper = 0.010084\n",
        "\n",
        "# Amplitude range.\n",
        "amp_lower = 1e-23\n",
        "amp_upper = 1e-21\n",
        "\n",
        "# Sample f0 and amp from a uniform prior.\n",
        "f0 = cp.random.uniform(f0_lower, f0_upper, num_samples)\n",
        "amp = cp.random.uniform(amp_lower, amp_upper, num_samples)\n"
      ],
      "metadata": {
        "id": "DJGtVdP8ibAC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed parameters\n",
        "ones = cp.ones(num_samples)\n",
        "fdot = 1.79e-15 * ones\n",
        "lam  = 4.36 * ones\n",
        "beta = 2.18 * ones\n",
        "iota = 0.67 * ones\n",
        "phi0 = 5.48 * ones\n",
        "psi  = 0.43 * ones"
      ],
      "metadata": {
        "id": "uziQ_gWRioPV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Package parameters into arrays.\n",
        "\n",
        "sampling_parameters = cp.vstack((f0, amp)).T\n",
        "all_parameters = cp.vstack((amp, f0, fdot, cp.zeros(num_samples), -phi0, iota, psi, lam, beta)).T"
      ],
      "metadata": {
        "id": "SUMiHBQ4ipUE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Waveform generation"
      ],
      "metadata": {
        "id": "Higr8WCr9DBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise waveform generator.\n",
        "gb = GBGPU(use_gpu=gpu)\n",
        "\n",
        "# Waveform settings\n",
        "Tobs = 31536000.0  # One-year observation\n",
        "dt = 15.0  # Sample rate (Nyquist is safely larger than the maximum frequency we will encounter)\n",
        "df = 1./Tobs\n",
        "N_points = 128\n",
        "\n",
        "# Generate the waveforms.\n",
        "gb.run_wave(*all_parameters.T, N = N_points, dt = dt, T = Tobs, oversample = 1)"
      ],
      "metadata": {
        "id": "6TAq9bILyLYy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The waveforms stored in gb are only defined over very narrow frequency support. They need to be stitched into a wider frequency band, which we define here. We choose a frequency range slightly larger than [f0_lower, f0_upper] such that full waveform can always fit in a band."
      ],
      "metadata": {
        "id": "Y4spZgLh9Rn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_min = 0.010059\n",
        "f_max = 0.0100861\n",
        "\n",
        "# Define the frequency grid.\n",
        "num_bins = int((f_max - f_min) / df) + 1\n",
        "sample_frequencies = cp.linspace(f_min, f_max, num=num_bins)"
      ],
      "metadata": {
        "id": "9uc8nscfyQRb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks perform best when inputs are standardized. For simulated data, we achieve this by whitening the waveforms using the predicted LISA power spectral density."
      ],
      "metadata": {
        "id": "MnCA7W3z9YBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise = AnalyticNoise(sample_frequencies)\n",
        "psd_A, psd_E = noise.psd(option=\"A\"), noise.psd(option=\"E\")\n",
        "\n",
        "asd_A = cp.sqrt(psd_A)\n",
        "asd_E = cp.sqrt(psd_E)"
      ],
      "metadata": {
        "id": "cYb9zhpfyRs-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(get_wrapper(sample_frequencies), get_wrapper(asd_A))\n",
        "plt.title('Amplitude spectral density in a band')\n",
        "plt.xlabel('frequency (Hz)')\n",
        "plt.ylabel('amplitude spectral density (channel A)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ntkkm-A9dkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we stitch the waveforms into the frequency grid, and at the same time apply the whitening."
      ],
      "metadata": {
        "id": "0izBtnhg9gOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_min = round(f_min/df)\n",
        "k_max = round(f_max/df)\n",
        "num = len(sample_frequencies)\n",
        "\n",
        "# These indices describe how to stitch the waveform into the larger frequency grid.\n",
        "i_start = (get_wrapper(gb.start_inds) - k_min).astype(cp.int32)\n",
        "i_end = (get_wrapper(gb.start_inds) - k_min + gb.N).astype(cp.int32)\n",
        "\n",
        "# PyTorch by default uses float32, and that should be sufficient for our purposes.\n",
        "# Here we use complex64 since the frequency-domain strain is complex.\n",
        "\n",
        "A_whitened = cp.empty((num_samples, num), dtype=cp.complex64)\n",
        "E_whitened = cp.empty((num_samples, num), dtype=cp.complex64)\n",
        "\n",
        "for i in range(num_samples):\n",
        "    x = cp.zeros(num, dtype=cp.complex128)\n",
        "    x[i_start[i]:i_end[i]] = gb.A[i]\n",
        "    x *= cp.sqrt(4 * df) / asd_A\n",
        "    A_whitened[i] = x\n",
        "\n",
        "    x = cp.zeros(num, dtype=cp.complex128)\n",
        "    x[i_start[i]:i_end[i]] = gb.E[i]\n",
        "    x *= cp.sqrt(4 * df) / asd_E\n",
        "    E_whitened[i] = x"
      ],
      "metadata": {
        "id": "TIEdWJazyXyH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a sample waveform\n",
        "plt.plot(get_wrapper(sample_frequencies), get_wrapper(A_whitened[0].real))\n",
        "plt.xscale('log')\n",
        "plt.xlabel('$f$')\n",
        "plt.ylabel('Re $A$')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4oFmgPrc9p0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Package into a pytorch Dataset\n",
        "\n",
        "The `Dataset` is a convenient class for storing and accessing pairs of parameters and associated data. It must define the following methods:\n",
        "\n",
        "* `__len__()`: Return total number of samples in the dataset.\n",
        "* `__getitem__(idx)`: Retrieve a $(\\theta, d)$ pair of parameters and data. We use this method to also add (in real time) a noise realization to each simulated waveform. (Therefore repeated calls will give different noise realizations).\n"
      ],
      "metadata": {
        "id": "w7FTYsnh90NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For best training, parameters should be standardized (zero mean, unit variance across the training set).\n",
        "\n",
        "parameters_mean = np.mean(sampling_parameters, axis=0)\n",
        "parameters_std = np.std(sampling_parameters, axis=0)\n",
        "\n",
        "parameters_standardized = (sampling_parameters - parameters_mean) / parameters_std\n",
        "parameters_standardized = parameters_standardized.astype(np.float32)"
      ],
      "metadata": {
        "id": "lQ0QNkWRycxY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the (complex) frequency-domain data for the (real) neural network. To do so we simply concatenate the real and imaginary parts into an array of doubled length.\n",
        "\n",
        "\n",
        "waveforms = cp.concatenate((cp.expand_dims(A_whitened, axis=1).real, cp.expand_dims(A_whitened, axis=1).imag,\n",
        "                            cp.expand_dims(E_whitened, axis=1).real, cp.expand_dims(E_whitened, axis=1).imag),axis=1)\n",
        "\n",
        "print(waveforms.shape)"
      ],
      "metadata": {
        "id": "Bt-qFQdcykcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WaveformDataset(Dataset):\n",
        "\n",
        "    def __init__(self, parameters, waveforms):\n",
        "        self.parameters = parameters\n",
        "        self.waveforms = waveforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.parameters)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        params = self.parameters[idx]\n",
        "        signal = self.waveforms[idx]\n",
        "\n",
        "        # Add unit normal noise to the signal\n",
        "        noise = cp.random.normal(size = signal.shape).astype(cp.float32)\n",
        "        data = signal + noise\n",
        "\n",
        "        return torch.tensor(data).type(dtype), torch.tensor(params).type(dtype)"
      ],
      "metadata": {
        "id": "2q0I95vQylS7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform_dataset = WaveformDataset(parameters_standardized, waveforms)"
      ],
      "metadata": {
        "id": "0jlJKNoByqXk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can sample from the WaveformDataset. This gives us pairs of data and parameters, different noise realizations each time.\n",
        "\n",
        "x, y = waveform_dataset[0]\n",
        "print(y)\n",
        "plt.plot(x[0].cpu())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rrv3BQ8Vyq9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Point parameter estimation.\n",
        "\n",
        "We now try to take the imput data and fit it to the corresponding parameters."
      ],
      "metadata": {
        "id": "Bxeo5vAE-rCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model construction\n",
        "\n",
        "We start with the simple CNN model which combines a set of different layers using `torch.nn.Sequential`"
      ],
      "metadata": {
        "id": "UFpH1N-B-g6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential(\n",
        "          nn.Conv1d(4,16,8,padding=\"same\"), # [input channels, number of filters, filter size]\n",
        "          nn.BatchNorm1d(16),\n",
        "          nn.Tanh(),\n",
        "          nn.MaxPool1d(2),\n",
        "          nn.Conv1d(16,64,8,padding=\"same\"),\n",
        "          nn.Tanh(),\n",
        "          nn.MaxPool1d(2),\n",
        "          nn.Conv1d(64,8,8,padding=\"same\"),\n",
        "          nn.Tanh(),\n",
        "          nn.MaxPool1d(2),\n",
        "          nn.Flatten(),\n",
        "          nn.LazyLinear(512), # LazyLinear automatically computes the input size from flattened convolutions\n",
        "          nn.Sigmoid(),\n",
        "          nn.LazyLinear(128),\n",
        "          nn.Sigmoid(),\n",
        "          nn.LazyLinear(2)\n",
        "        ).to(dev)\n"
      ],
      "metadata": {
        "id": "kJj2bNhGj-4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "320KSVhI_wgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and test sets. We use the test set to make sure the network properly generalizes to data that it has not seen in training, i.e., it does not overfit.\n",
        "\n",
        "train_fraction = 0.8\n",
        "num_train = int(round(train_fraction * num_samples))\n",
        "num_test = num_samples - num_train\n",
        "train_dataset, test_dataset = random_split(waveform_dataset, [num_train, num_test])\n",
        "\n",
        "# The DataLoader is used in training.\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "PkDpbhAly5M8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The DataLoaders iterate over samples, returning torch tensors containing a batch of data.\n",
        "\n",
        "train_features, train_labels = next(iter(train_dataloader))\n"
      ],
      "metadata": {
        "id": "9gKgIo9AzGDe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the Adam optimizer.\n",
        "\n",
        "optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad==True], lr=1e-4)"
      ],
      "metadata": {
        "id": "BLaeanWc0u3U"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define loss function"
      ],
      "metadata": {
        "id": "DmYx-P1nACWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use Meas Square Error.\n",
        "\n",
        "loss_func = torch.nn.MSELoss()\n"
      ],
      "metadata": {
        "id": "5hcLcs5tkDpj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and test loops.\n",
        "\n",
        "def train_loop(dataloader, model, optimizer):\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        outputs = model(X)\n",
        "        loss = loss_func(outputs, y)\n",
        "\n",
        "        train_loss += loss.detach().sum()\n",
        "        loss = loss.mean()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 20 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d} samples]\")\n",
        "\n",
        "    average_loss = train_loss.item() / size\n",
        "    print('Average loss: {:.4f}'.format(average_loss))\n",
        "    return average_loss\n"
      ],
      "metadata": {
        "id": "IMBI_69aAL9e"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(dataloader, model):\n",
        "    size = len(dataloader.dataset)\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            outputs = model(X)\n",
        "            loss = loss_func(outputs, y)\n",
        "            test_loss += loss.sum()\n",
        "\n",
        "    test_loss /= size\n",
        "    print(f\"Test loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss"
      ],
      "metadata": {
        "id": "NWqAh2UdAbOm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "train_history = []\n",
        "test_history = []\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    loss = train_loop(train_dataloader, model, optimizer)\n",
        "    train_history.append(loss)\n",
        "    loss = test_loop(test_dataloader, model)\n",
        "    test_history.append(loss.cpu())\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "tiXvICTZAls2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.arange(1, len(train_history) + 1)\n",
        "print(type(test_history))\n",
        "plt.plot(epochs, train_history, label = 'train loss')\n",
        "plt.plot(epochs, test_history, label = 'test loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JzQ0fXpbCHqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features, test_labels = next(iter(test_dataloader))\n",
        "test_outputs = model(test_features)\n"
      ],
      "metadata": {
        "id": "By7JG7cCMy5K"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(test_labels[:,0], test_outputs[:,0].detach().numpy(), label = '$f0$')\n",
        "plt.scatter(test_labels[:,1], test_outputs[:,1].detach().numpy(), label = 'amplitude')\n",
        "plt.xlabel('True value of parameter (standerdized)')\n",
        "plt.ylabel('Predicted value of parameter (standerdized)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vc3d535mdyv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_posteriors = 5\n",
        "num_samples = 10000\n",
        "\n",
        "for n in range(num_posteriors):\n",
        "    test_x, test_y = test_dataset[n]\n",
        "\n",
        "    # Repeat same wf for a number of samples\n",
        "    wf = torch.tile(test_x, (num_samples, 1))\n",
        "\n",
        "    # Predict a posterior\n",
        "    pred_samples = cp.asarray(model(test_features).detach())\n",
        "\n",
        "    # Undo the standardization\n",
        "    pred_samples = parameters_std * pred_samples + parameters_mean\n",
        "    truth = parameters_std * cp.asarray(test_y) + parameters_mean\n",
        "\n",
        "    # Plot\n",
        "    corner.corner(get_wrapper(pred_samples), truths=get_wrapper(truth), labels=['$f0$', '$amp$'])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "P3PW_PVnYkFl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}