{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf826081-c9ef-4aff-ba53-8e9b573ca2d8",
   "metadata": {},
   "source": [
    "# Tutorial on Machine Learning for Gravitational Wave Parameter Estimation\n",
    "\n",
    "### Stephen Green *stephen.green2@nottingham.ac.uk* and Natalia Korsakova *korsakova@apc.in2p3.fr*\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial we will build a simple **parameter estimation** neural network:\n",
    "* **Training data:** TaylorF2 waveforms, high SNR, parametrized only by masses; noise added during training\n",
    "* **Posterior model:** Gaussian with learnable (diagonal) covariance matrix\n",
    "\n",
    "This should run in about a minute on a laptop.\n",
    "\n",
    "### Exercises\n",
    "1. Add new parameters, beyond the masses\n",
    "2. Extend the Gaussian distribution to include general covariance\n",
    "3. Make a PP plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213733c5-f5c3-484b-a9b0-f15f4a8fb0c3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f07d8-352e-4bda-ac61-69356a965e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycbc.waveform import get_fd_waveform, fd_approximants\n",
    "from pycbc.filter import match\n",
    "from pycbc.psd import aLIGOZeroDetHighPower\n",
    "from pycbc.types import FrequencySeries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lal\n",
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa639f0f-2579-4243-aae2-52180187d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181558eb-0df5-49bb-b773-4d3de235f767",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3836cd8-96a4-4adc-8416-770b30b4e07b",
   "metadata": {},
   "source": [
    "Generate a training set that (for simplicity) samples only over the component masses. Generate frequency-domain waveforms using TaylorF2. We will add noise during training.\n",
    "\n",
    "**Exercise:** Add more parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49802b93-371b-4463-9469-7d3ce76d9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000  # size of the training set\n",
    "\n",
    "m_lower = 20.0  # solar masses\n",
    "m_upper = 80.0\n",
    "masses = m_lower + np.random.random((num_samples, 2)) * (m_upper - m_lower)\n",
    "\n",
    "# Make sure m1 > m2\n",
    "masses = np.sort(masses, axis=-1)\n",
    "masses = np.flip(masses, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264c0ba-c1bc-4bd6-9931-b6d2bea4c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f2fda-270f-4895-8dc6-23079de2b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters\n",
    "\n",
    "distance = 1000  # Mpc\n",
    "inclination = 0.0  # face on\n",
    "spin1x = 0.0\n",
    "spin1y = 0.0\n",
    "spin1z = 0.0\n",
    "spin2x = 0.0\n",
    "spin2y = 0.0\n",
    "spin2z = 0.0\n",
    "phase = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb451fda-85f5-42d4-93db-0fc0d5fe3dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waveform settings\n",
    "\n",
    "approximant = 'TaylorF2'\n",
    "f_lower = 20.0\n",
    "f_final = 1024.0\n",
    "T = 1.0\n",
    "\n",
    "delta_f = 1 / T\n",
    "nf = int(f_final / delta_f) + 1\n",
    "f_array = np.linspace(0.0, f_final, num=nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c9114-8c49-4e58-b91d-04ddedd3047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise PSD\n",
    "\n",
    "psd = np.array(aLIGOZeroDetHighPower(nf, delta_f, 0.0))\n",
    "psd[0] = psd[1]  # Fix up endpoints\n",
    "psd[-1] = psd[-2]\n",
    "\n",
    "asd = np.sqrt(psd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2aeb73-a658-4804-b528-a0e3abf302bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training waveforms\n",
    "\n",
    "hp_list = []\n",
    "hc_list = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    \n",
    "    mass1, mass2 = masses[i]\n",
    "    \n",
    "    hp, hc = get_fd_waveform(approximant=approximant,\n",
    "                             mass1=mass1, mass2=mass2,\n",
    "                             inclination=inclination,\n",
    "                             distance=distance,\n",
    "                             coa_phase = phase,\n",
    "                             spin1x=spin1x, spin1y=spin1y, spin1z=spin1z,\n",
    "                             spin2x=spin2x, spin2y=spin2y, spin2z=spin2z,        \n",
    "                             delta_f=delta_f,\n",
    "                             f_lower=f_lower, f_final=f_final)\n",
    "    \n",
    "    f_array = np.array(hp.sample_frequencies)\n",
    "    \n",
    "    # Whiten waveforms and rescale so that white noise has unit variance\n",
    "    hp = hp / asd * np.sqrt(4.0 * delta_f)\n",
    "    hc = hc / asd * np.sqrt(4.0 * delta_f)\n",
    "    \n",
    "    hp_list.append(hp)\n",
    "    hc_list.append(hc)\n",
    "\n",
    "hp = np.array(hp_list)\n",
    "hc = np.array(hc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b603a8-ce6e-4202-a2e5-c85175fb308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample waveform\n",
    "\n",
    "plt.plot(f_array, hp[0].real)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$f$')\n",
    "plt.ylabel('Re $h_+$')\n",
    "plt.xlim((10, f_final))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a557d-38c1-41a6-a038-419f0d13425d",
   "metadata": {},
   "source": [
    "### Package into a pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f457f-3256-43c8-9e99-9ee958462c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "#\n",
    "# This is just the masses. It's better to sample in (Mc, q) rather than (m1, m2) because the posterior is more Gaussian\n",
    "\n",
    "m1 = masses[:, 0]\n",
    "m2 = masses[:, 1]\n",
    "\n",
    "Mc = (m1 * m2)**(3/5) / (m1 + m2)**(1/5)\n",
    "q = m2 / m1\n",
    "\n",
    "parameters = np.stack((Mc, q), axis=1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a1d63-01d0-42db-bbf2-7db0adc598a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For best training, parameters should be standardized (zero mean, unit variance across the training set)\n",
    "\n",
    "parameters_mean = np.mean(parameters, axis=0)\n",
    "parameters_std = np.std(parameters, axis=0)\n",
    "\n",
    "parameters_standardized = (parameters - parameters_mean) / parameters_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6bd1a-040f-4bf1-99c9-284ce01fa049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waveforms\n",
    "#\n",
    "# Truncate the arrays to remove zeros below f_lower, and repackage real and imaginary parts\n",
    "#\n",
    "# Only consider h_plus for now\n",
    "\n",
    "lower_cut = int(f_lower / delta_f)\n",
    "waveforms = np.hstack((hp.real[:, lower_cut:], hp.imag[:, lower_cut:])).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee096ac-93b8-4725-bec5-2268ab416d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveformDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, parameters, waveforms):\n",
    "        self.parameters = parameters\n",
    "        self.waveforms = waveforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parameters)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        params = self.parameters[idx]\n",
    "        signal = self.waveforms[idx]\n",
    "        \n",
    "        # Add unit normal noise to the signal\n",
    "        noise = np.random.normal(size = signal.shape).astype(np.float32)\n",
    "        data = signal + noise\n",
    "        \n",
    "        return torch.tensor(data), torch.tensor(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962f84b-f12b-42e8-ad63-d713fd026c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_dataset = WaveformDataset(parameters_standardized, waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b3df66-380e-475e-805e-bd8c0adb32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sample from the WaveformDataset. This gives us pairs of data and parameters, different noise realizations each time.\n",
    "\n",
    "x, y = waveform_dataset[0]\n",
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a15e1-7686-4f4d-a02c-23b6d035833a",
   "metadata": {},
   "source": [
    "## Posterior Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d2fb4-969c-4c83-ba1d-ffcb0024e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural networks are constructed by subclassing nn.Module\n",
    "#\n",
    "# This has to implement an __init__() and forward() method\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, activation=nn.ReLU()):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_net_list = []\n",
    "        hidden_net_list.append(\n",
    "            nn.Linear(input_dim, hidden_dims[0]))\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            hidden_net_list.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "        self.hidden_net_list = nn.ModuleList(hidden_net_list)\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_mean = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.output_log_sigma = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass x through all the layers of the network and return the Gaussian distribution\"\"\"\n",
    "        \n",
    "        h = x\n",
    "        for layer in self.hidden_net_list:\n",
    "            h = self.activation(layer(h))\n",
    "\n",
    "        # Output layer defines a Gaussian\n",
    "        mean = self.output_mean(h)\n",
    "        log_sigma = self.output_log_sigma(h)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        \n",
    "        # Create the Gaussian distribution\n",
    "        dist = torch.distributions.MultivariateNormal(loc=mean, scale_tril=torch.diag_embed(sigma))\n",
    "        \n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cdfa4-1fe8-4993-a5f0-311889bba38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = waveforms.shape[-1]\n",
    "output_dim = parameters.shape[-1]\n",
    "hidden_dims = [512, 256, 128, 64, 32]\n",
    "\n",
    "model = NeuralNetwork(input_dim, hidden_dims, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d8148-32b1-4a46-952f-7625d1a32eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee627d9-db98-4d9f-82f6-99e51264de02",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f5f26-796f-484c-b52c-a035090ecb02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "\n",
    "train_fraction = 0.8\n",
    "num_train = int(round(train_fraction * num_samples))\n",
    "num_test = num_samples - num_train\n",
    "train_dataset, test_dataset = random_split(waveform_dataset, [num_train, num_test])\n",
    "\n",
    "# The DataLoader is used in training\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16304af3-c970-416a-8520-1db669330f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoaders iterate over samples, returning torch tensors containing a batch of data\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502834d2-fb64-484a-8fa6-3616e3633a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5541e3-1a5a-435c-b0f2-98b8bddcfb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8e2e9-f485-485d-8bc2-fc31d74afe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Adam optimizer.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b543d-e129-4845-ace5-f6a387d6ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test loops\n",
    "\n",
    "def train_loop(dataloader, model, optimizer):\n",
    " \n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute negative log probability loss\n",
    "        dist = model(X)        \n",
    "        loss = - dist.log_prob(y)\n",
    "        \n",
    "        train_loss += loss.detach().sum()\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d} samples]\")\n",
    "            \n",
    "    average_loss = train_loss.item() / size\n",
    "    print('Average loss: {:.4f}'.format(average_loss))\n",
    "    return average_loss\n",
    "            \n",
    "        \n",
    "        \n",
    "def test_loop(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            dist = model(X)\n",
    "            loss = - dist.log_prob(y)\n",
    "            test_loss += loss.sum()\n",
    "\n",
    "    test_loss /= size\n",
    "    print(f\"Test loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8d1ff-461a-4e1b-a11f-82946561b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "train_history = []\n",
    "test_history = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    loss = train_loop(train_dataloader, model, optimizer)\n",
    "    train_history.append(loss)\n",
    "    loss = test_loop(test_dataloader, model)\n",
    "    test_history.append(loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2c6a2-9541-408a-9ff8-b2273da49e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(train_history) + 1)\n",
    "plt.plot(epochs, train_history, label = 'train loss')\n",
    "plt.plot(epochs, test_history, label = 'test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d14a1-eb3f-4bbb-8c84-afee8077f928",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7639f37-f16d-4a64-b626-ed30d0111397",
   "metadata": {},
   "source": [
    "### Posterior plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c6a64-cee3-4ff8-9683-59fd24abe223",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posteriors = 10\n",
    "num_samples = 10000\n",
    "\n",
    "for n in range(num_posteriors):\n",
    "    test_x, test_y = test_dataset[n]\n",
    "\n",
    "    # Predict a posterior\n",
    "    dist = model(test_x)\n",
    "\n",
    "    # Sample the posterior\n",
    "    pred_samples = dist.sample((10000,)).numpy()\n",
    "\n",
    "    # Undo the standardization\n",
    "    pred_samples = parameters_std * pred_samples + parameters_mean\n",
    "    truth = parameters_std * test_y.numpy() + parameters_mean\n",
    "\n",
    "    # Plot\n",
    "    corner.corner(pred_samples, truths=truth, labels=['$M_c$', '$q$'])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
