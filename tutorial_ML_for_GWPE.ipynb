{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataliaKor/tutorial/blob/main/tutorial_ML_for_GWPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf826081-c9ef-4aff-ba53-8e9b573ca2d8",
      "metadata": {
        "id": "cf826081-c9ef-4aff-ba53-8e9b573ca2d8"
      },
      "source": [
        "# Tutorial on Machine Learning for Gravitational Wave Parameter Estimation\n",
        "\n",
        "### Stephen Green *stephen.green2@nottingham.ac.uk* and Natalia Korsakova *korsakova@apc.in2p3.fr*\n",
        "\n",
        "---\n",
        "\n",
        "In this tutorial we will build a simple **parameter estimation** neural network:\n",
        "* **Training data:** TaylorF2 waveforms, high SNR, parametrized only by masses; noise added during training\n",
        "* **Posterior model:** Gaussian with learnable (diagonal) covariance matrix\n",
        "\n",
        "This should run in about a minute on a laptop.\n",
        "\n",
        "### Exercises\n",
        "1. Add new parameters, beyond the masses\n",
        "2. Extend the Gaussian distribution to include general covariance\n",
        "3. Make a PP plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "213733c5-f5c3-484b-a9b0-f15f4a8fb0c3",
      "metadata": {
        "id": "213733c5-f5c3-484b-a9b0-f15f4a8fb0c3"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install corner"
      ],
      "metadata": {
        "id": "qxk8TFdoBP2Y",
        "outputId": "2c0e23fd-1e8c-4c70-b9ff-7f0470408af9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qxk8TFdoBP2Y",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting corner\n",
            "  Downloading corner-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: matplotlib>=2.1 in /usr/local/lib/python3.7/dist-packages (from corner) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->corner) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->corner) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->corner) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->corner) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->corner) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.1->corner) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1->corner) (1.15.0)\n",
            "Installing collected packages: corner\n",
            "Successfully installed corner-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f04f07d8-352e-4bda-ac61-69356a965e51",
      "metadata": {
        "id": "f04f07d8-352e-4bda-ac61-69356a965e51"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import corner\n",
        "# Make a check if CPU use np and if GPU use cupy\n",
        "import cupy as cp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "aa639f0f-2579-4243-aae2-52180187d97b",
      "metadata": {
        "id": "aa639f0f-2579-4243-aae2-52180187d97b"
      },
      "outputs": [],
      "source": [
        "# pytorch imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NataliaKor/GBGPU.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2xwybJfHGvQ",
        "outputId": "d1ddcd68-6c43-405e-faf7-6853d7464221"
      },
      "id": "A2xwybJfHGvQ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GBGPU'...\n",
            "remote: Enumerating objects: 1120, done.\u001b[K\n",
            "remote: Counting objects: 100% (294/294), done.\u001b[K\n",
            "remote: Compressing objects: 100% (196/196), done.\u001b[K\n",
            "remote: Total 1120 (delta 140), reused 203 (delta 97), pack-reused 826\u001b[K\n",
            "Receiving objects: 100% (1120/1120), 12.08 MiB | 17.92 MiB/s, done.\n",
            "Resolving deltas: 100% (630/630), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from GBGPU.gbgpu.gbgpu import GBGPU\n",
        "from GBGPU.gbgpu.noisemodel import AnalyticNoise"
      ],
      "metadata": {
        "id": "ZF-3zFO1FBrr"
      },
      "id": "ZF-3zFO1FBrr",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_points = None\n",
        "\n",
        "# Create waveform\n",
        "gb = GBGPU(use_gpu=True)\n"
      ],
      "metadata": {
        "id": "Xl8hcdznEfbo"
      },
      "id": "Xl8hcdznEfbo",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "181558eb-0df5-49bb-b773-4d3de235f767",
      "metadata": {
        "id": "181558eb-0df5-49bb-b773-4d3de235f767"
      },
      "source": [
        "## Training data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3836cd8-96a4-4adc-8416-770b30b4e07b",
      "metadata": {
        "id": "d3836cd8-96a4-4adc-8416-770b30b4e07b"
      },
      "source": [
        "Generate a training set that (for simplicity) samples only over amplitudes and frequencies. Generate frequency-domain waveforms using FastGB. We will add noise during training.\n",
        "\n",
        "**Exercise:** Add more parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49802b93-371b-4463-9469-7d3ce76d9816",
      "metadata": {
        "id": "49802b93-371b-4463-9469-7d3ce76d9816"
      },
      "outputs": [],
      "source": [
        "num_samples = 10000  # size of the training set\n",
        "\n",
        "# Choose something in the lower frequencies\n",
        "f0_min = 0.010062\n",
        "f0_max = 0.010085\n",
        "\n",
        "amp_min = -23.\n",
        "amp_max = -21.\n",
        "\n",
        "# frequency\n",
        "f0 = np.random.uniform(f0_min, f0_max, num_samples)\n",
        "\n",
        "# amplitude\n",
        "amp = 10**np.random.uniform(amp_min, amp_max, num_samples)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4264c0ba-c1bc-4bd6-9931-b6d2bea4c898",
      "metadata": {
        "id": "4264c0ba-c1bc-4bd6-9931-b6d2bea4c898"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "658f2fda-270f-4895-8dc6-23079de2b42a",
      "metadata": {
        "id": "658f2fda-270f-4895-8dc6-23079de2b42a"
      },
      "outputs": [],
      "source": [
        "# Fixed parameters\n",
        "\n",
        "fdot = 1.79e-15\n",
        "lam  = 4.36\n",
        "beta = 2.18\n",
        "iota = 0.67\n",
        "phi0 = 5.48\n",
        "psi  = 0.43\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb451fda-85f5-42d4-93db-0fc0d5fe3dc5",
      "metadata": {
        "id": "eb451fda-85f5-42d4-93db-0fc0d5fe3dc5"
      },
      "outputs": [],
      "source": [
        "# Waveform settings\n",
        "\n",
        "\n",
        "Tobs = 31536000.0\n",
        "dt = 15.0\n",
        "df = 1./Tobs\n",
        "\n",
        "# Put the waveform in the common frequency band \n",
        "k_min = np.round(fvec_min/df).astype(int)\n",
        "k_max = np.round(fvec_max/df).astype(int)\n",
        "num = k_max - k_min\n",
        "freqs = d(np.arange(num) + k_min)*df\n",
        "\n",
        "#f_lower = 20.0\n",
        "#f_final = 1024.0\n",
        "#T = 1.0\n",
        "\n",
        "#delta_f = 1 / T\n",
        "#nf = int(f_final / delta_f) + 1\n",
        "#f_array = np.linspace(0.0, f_final, num=nf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6c9114-8c49-4e58-b91d-04ddedd3047a",
      "metadata": {
        "id": "5a6c9114-8c49-4e58-b91d-04ddedd3047a"
      },
      "outputs": [],
      "source": [
        "# Noise PSD\n",
        "\n",
        "psd = AnalyticNoise(freqs)\n",
        "psdA, psdE = noise.psd(option=\"A\"), noise.psd(option=\"E\")\n",
        "\n",
        "#psd[0] = psd[1]  # Fix up endpoints\n",
        "#psd[-1] = psd[-2]\n",
        "\n",
        "asdA = np.sqrt(psdA)\n",
        "asdE = np.sqrt(psdE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2aeb73-a658-4804-b528-a0e3abf302bf",
      "metadata": {
        "id": "5a2aeb73-a658-4804-b528-a0e3abf302bf"
      },
      "outputs": [],
      "source": [
        "# Generate training waveforms\n",
        "\n",
        "A_list = []\n",
        "E_list = []\n",
        "\n",
        "for i in range(num_samples):\n",
        "    \n",
        "    #mass1, mass2 = masses[i]\n",
        "\n",
        "    params = np.array([amp[i], f0[i], fdot, 0.0, -phi0, iota, psi, lam, beta])\n",
        "    gb.run_wave(*params, N = N_points, dt = dt, T = Tobs, oversample = 1)#oversample=2)\n",
        "    \n",
        "    #hp, hc = get_fd_waveform(approximant=approximant,\n",
        "    #                         mass1=mass1, mass2=mass2,\n",
        "    #                         inclination=inclination,\n",
        "    #                         distance=distance,\n",
        "    #                         coa_phase = phase,\n",
        "    #                         spin1x=spin1x, spin1y=spin1y, spin1z=spin1z,\n",
        "    #                         spin2x=spin2x, spin2y=spin2y, spin2z=spin2z,        \n",
        "    #                         delta_f=delta_f,\n",
        "    #                         f_lower=f_lower, f_final=f_final)\n",
        "    \n",
        "    #f_array = np.array(hp.sample_frequencies)\n",
        "    \n",
        "    i_start = (gb.start_inds.get() - k_min).astype(np.int32)\n",
        "    i_end = (gb.start_inds.get() - k_min + gb.N).astype(np.int32)\n",
        "\n",
        "    A_out = cp.zeros((1, num), dtype=cp.complex128)\n",
        "    E_out = cp.zeros((1, num), dtype=cp.complex128)\n",
        "\n",
        "    #Â Put waveforms in the same frequency range\n",
        "    A_out[i_start : i_end] = gb.A\n",
        "    E_out[i_start : i_end] = gb.E\n",
        "\n",
        "    # Whiten waveforms and rescale so that white noise has unit variance\n",
        "    #hp = hp / asd * np.sqrt(4.0 * delta_f)\n",
        "    #hc = hc / asd * np.sqrt(4.0 * delta_f)\n",
        "    A_white = A_out * cp.sqrt(4.0 * df)* dt/cp.array(asdA)\n",
        "    E_white = E_out * cp.sqrt(4.0 * df)* dt/cp.array(asdE)\n",
        "\n",
        "    A_list.append(A_white)\n",
        "    E_list.append(E_white)\n",
        "\n",
        "Awf = np.array(A_list)\n",
        "Ewf = np.array(E_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93b603a8-ce6e-4202-a2e5-c85175fb308c",
      "metadata": {
        "id": "93b603a8-ce6e-4202-a2e5-c85175fb308c"
      },
      "outputs": [],
      "source": [
        "# Sample waveform\n",
        "\n",
        "plt.plot(f_array, Awf[0].real)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('$f$')\n",
        "plt.ylabel('Re $A$')\n",
        "#plt.xlim((10, f_final))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b6a557d-38c1-41a6-a038-419f0d13425d",
      "metadata": {
        "id": "2b6a557d-38c1-41a6-a038-419f0d13425d"
      },
      "source": [
        "### Package into a pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "921f457f-3256-43c8-9e99-9ee958462c65",
      "metadata": {
        "id": "921f457f-3256-43c8-9e99-9ee958462c65"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "#\n",
        "# This is just the masses. It's better to sample in (Mc, q) rather than (m1, m2) because the posterior is more Gaussian\n",
        "\n",
        "m1 = masses[:, 0]\n",
        "m2 = masses[:, 1]\n",
        "\n",
        "Mc = (m1 * m2)**(3/5) / (m1 + m2)**(1/5)\n",
        "q = m2 / m1\n",
        "\n",
        "parameters = np.stack((Mc, q), axis=1).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583a1d63-01d0-42db-bbf2-7db0adc598a9",
      "metadata": {
        "id": "583a1d63-01d0-42db-bbf2-7db0adc598a9"
      },
      "outputs": [],
      "source": [
        "# For best training, parameters should be standardized (zero mean, unit variance across the training set)\n",
        "\n",
        "parameters_mean = np.mean(parameters, axis=0)\n",
        "parameters_std = np.std(parameters, axis=0)\n",
        "\n",
        "parameters_standardized = (parameters - parameters_mean) / parameters_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b6bd1a-040f-4bf1-99c9-284ce01fa049",
      "metadata": {
        "id": "27b6bd1a-040f-4bf1-99c9-284ce01fa049"
      },
      "outputs": [],
      "source": [
        "# Waveforms\n",
        "#\n",
        "# Truncate the arrays to remove zeros below f_lower, and repackage real and imaginary parts\n",
        "#\n",
        "# Only consider h_plus for now\n",
        "\n",
        "lower_cut = int(f_lower / delta_f)\n",
        "waveforms = np.hstack((hp.real[:, lower_cut:], hp.imag[:, lower_cut:])).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ee096ac-93b8-4725-bec5-2268ab416d37",
      "metadata": {
        "id": "5ee096ac-93b8-4725-bec5-2268ab416d37"
      },
      "outputs": [],
      "source": [
        "class WaveformDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, parameters, waveforms):\n",
        "        self.parameters = parameters\n",
        "        self.waveforms = waveforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.parameters)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        params = self.parameters[idx]\n",
        "        signal = self.waveforms[idx]\n",
        "        \n",
        "        # Add unit normal noise to the signal\n",
        "        noise = np.random.normal(size = signal.shape).astype(np.float32)\n",
        "        data = signal + noise\n",
        "        \n",
        "        return torch.tensor(data), torch.tensor(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9962f84b-f12b-42e8-ad63-d713fd026c66",
      "metadata": {
        "id": "9962f84b-f12b-42e8-ad63-d713fd026c66"
      },
      "outputs": [],
      "source": [
        "waveform_dataset = WaveformDataset(parameters_standardized, waveforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94b3df66-380e-475e-805e-bd8c0adb32dc",
      "metadata": {
        "id": "94b3df66-380e-475e-805e-bd8c0adb32dc"
      },
      "outputs": [],
      "source": [
        "# We can sample from the WaveformDataset. This gives us pairs of data and parameters, different noise realizations each time.\n",
        "\n",
        "x, y = waveform_dataset[0]\n",
        "plt.plot(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5a15e1-7686-4f4d-a02c-23b6d035833a",
      "metadata": {
        "id": "cd5a15e1-7686-4f4d-a02c-23b6d035833a"
      },
      "source": [
        "## Posterior Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a30d2fb4-969c-4c83-ba1d-ffcb0024e6a1",
      "metadata": {
        "id": "a30d2fb4-969c-4c83-ba1d-ffcb0024e6a1"
      },
      "outputs": [],
      "source": [
        "# Neural networks are constructed by subclassing nn.Module\n",
        "#\n",
        "# This has to implement an __init__() and forward() method\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, activation=nn.ReLU()):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        \n",
        "        # Hidden layers\n",
        "        hidden_net_list = []\n",
        "        hidden_net_list.append(\n",
        "            nn.Linear(input_dim, hidden_dims[0]))\n",
        "        for i in range(1, len(hidden_dims)):\n",
        "            hidden_net_list.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
        "        self.hidden_net_list = nn.ModuleList(hidden_net_list)\n",
        "        \n",
        "        # Output layers\n",
        "        self.output_mean = nn.Linear(hidden_dims[-1], output_dim)\n",
        "        self.output_log_sigma = nn.Linear(hidden_dims[-1], output_dim)\n",
        "        \n",
        "        # Activation function\n",
        "        self.activation = activation\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"Pass x through all the layers of the network and return the Gaussian distribution\"\"\"\n",
        "        \n",
        "        h = x\n",
        "        for layer in self.hidden_net_list:\n",
        "            h = self.activation(layer(h))\n",
        "\n",
        "        # Output layer defines a Gaussian\n",
        "        mean = self.output_mean(h)\n",
        "        log_sigma = self.output_log_sigma(h)\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        \n",
        "        # Create the Gaussian distribution\n",
        "        dist = torch.distributions.MultivariateNormal(loc=mean, scale_tril=torch.diag_embed(sigma))\n",
        "        \n",
        "        return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7cdfa4-1fe8-4993-a5f0-311889bba38d",
      "metadata": {
        "id": "6d7cdfa4-1fe8-4993-a5f0-311889bba38d"
      },
      "outputs": [],
      "source": [
        "input_dim = waveforms.shape[-1]\n",
        "output_dim = parameters.shape[-1]\n",
        "hidden_dims = [512, 256, 128, 64, 32]\n",
        "\n",
        "model = NeuralNetwork(input_dim, hidden_dims, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b59d8148-32b1-4a46-952f-7625d1a32eac",
      "metadata": {
        "id": "b59d8148-32b1-4a46-952f-7625d1a32eac"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee627d9-db98-4d9f-82f6-99e51264de02",
      "metadata": {
        "id": "7ee627d9-db98-4d9f-82f6-99e51264de02"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "232f5f26-796f-484c-b52c-a035090ecb02",
      "metadata": {
        "tags": [],
        "id": "232f5f26-796f-484c-b52c-a035090ecb02"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and test sets\n",
        "\n",
        "train_fraction = 0.8\n",
        "num_train = int(round(train_fraction * num_samples))\n",
        "num_test = num_samples - num_train\n",
        "train_dataset, test_dataset = random_split(waveform_dataset, [num_train, num_test])\n",
        "\n",
        "# The DataLoader is used in training\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16304af3-c970-416a-8520-1db669330f4c",
      "metadata": {
        "id": "16304af3-c970-416a-8520-1db669330f4c"
      },
      "outputs": [],
      "source": [
        "# The DataLoaders iterate over samples, returning torch tensors containing a batch of data\n",
        "\n",
        "train_features, train_labels = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "502834d2-fb64-484a-8fa6-3616e3633a87",
      "metadata": {
        "id": "502834d2-fb64-484a-8fa6-3616e3633a87"
      },
      "outputs": [],
      "source": [
        "train_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef5541e3-1a5a-435c-b0f2-98b8bddcfb9e",
      "metadata": {
        "id": "ef5541e3-1a5a-435c-b0f2-98b8bddcfb9e"
      },
      "outputs": [],
      "source": [
        "train_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ff8e2e9-f485-485d-8bc2-fc31d74afe18",
      "metadata": {
        "id": "3ff8e2e9-f485-485d-8bc2-fc31d74afe18"
      },
      "outputs": [],
      "source": [
        "# We use the Adam optimizer.\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "594b543d-e129-4845-ace5-f6a387d6ad41",
      "metadata": {
        "id": "594b543d-e129-4845-ace5-f6a387d6ad41"
      },
      "outputs": [],
      "source": [
        "# Training and test loops\n",
        "\n",
        "def train_loop(dataloader, model, optimizer):\n",
        " \n",
        "    size = len(dataloader.dataset)\n",
        "    train_loss = 0\n",
        "    \n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute negative log probability loss\n",
        "        dist = model(X)        \n",
        "        loss = - dist.log_prob(y)\n",
        "        \n",
        "        train_loss += loss.detach().sum()\n",
        "        loss = loss.mean()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d} samples]\")\n",
        "            \n",
        "    average_loss = train_loss.item() / size\n",
        "    print('Average loss: {:.4f}'.format(average_loss))\n",
        "    return average_loss\n",
        "            \n",
        "        \n",
        "        \n",
        "def test_loop(dataloader, model):\n",
        "    size = len(dataloader.dataset)\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            dist = model(X)\n",
        "            loss = - dist.log_prob(y)\n",
        "            test_loss += loss.sum()\n",
        "\n",
        "    test_loss /= size\n",
        "    print(f\"Test loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a8d1ff-461a-4e1b-a11f-82946561b876",
      "metadata": {
        "id": "19a8d1ff-461a-4e1b-a11f-82946561b876"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "train_history = []\n",
        "test_history = []\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    loss = train_loop(train_dataloader, model, optimizer)\n",
        "    train_history.append(loss)\n",
        "    loss = test_loop(test_dataloader, model)\n",
        "    test_history.append(loss)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e2c6a2-9541-408a-9ff8-b2273da49e09",
      "metadata": {
        "id": "33e2c6a2-9541-408a-9ff8-b2273da49e09"
      },
      "outputs": [],
      "source": [
        "epochs = np.arange(1, len(train_history) + 1)\n",
        "plt.plot(epochs, train_history, label = 'train loss')\n",
        "plt.plot(epochs, test_history, label = 'test loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c9d14a1-eb3f-4bbb-8c84-afee8077f928",
      "metadata": {
        "id": "6c9d14a1-eb3f-4bbb-8c84-afee8077f928"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7639f37-f16d-4a64-b626-ed30d0111397",
      "metadata": {
        "id": "c7639f37-f16d-4a64-b626-ed30d0111397"
      },
      "source": [
        "### Posterior plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "509c6a64-cee3-4ff8-9683-59fd24abe223",
      "metadata": {
        "id": "509c6a64-cee3-4ff8-9683-59fd24abe223"
      },
      "outputs": [],
      "source": [
        "num_posteriors = 10\n",
        "num_samples = 10000\n",
        "\n",
        "for n in range(num_posteriors):\n",
        "    test_x, test_y = test_dataset[n]\n",
        "\n",
        "    # Predict a posterior\n",
        "    dist = model(test_x)\n",
        "\n",
        "    # Sample the posterior\n",
        "    pred_samples = dist.sample((10000,)).numpy()\n",
        "\n",
        "    # Undo the standardization\n",
        "    pred_samples = parameters_std * pred_samples + parameters_mean\n",
        "    truth = parameters_std * test_y.numpy() + parameters_mean\n",
        "\n",
        "    # Plot\n",
        "    corner.corner(pred_samples, truths=truth, labels=['$M_c$', '$q$'])\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}